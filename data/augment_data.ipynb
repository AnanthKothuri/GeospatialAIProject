{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Event Data Processing\n",
    "\n",
    "This notebook processes storm event CSV files to extract and filter flooding events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading initial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(data_dir: str) -> List[str]:\n",
    "    csv_files = []\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Directory {data_dir} does not exist\")\n",
    "    \n",
    "    for file in data_path.glob(\"*.csv\"):\n",
    "        csv_files.append(str(file))\n",
    "    \n",
    "    return sorted(csv_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_file(file_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_flooding_events(df: pd.DataFrame, state: str = None) -> pd.DataFrame:\n",
    "    if df.empty or 'EVENT_TYPE' not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    flood_keywords = ['flood', 'flash flood', 'coastal flood', 'lakeshore flood', \n",
    "                      'river flood', 'urban flood', 'small stream flood']\n",
    "    \n",
    "    mask = df['EVENT_TYPE'].str.lower().str.contains('|'.join(flood_keywords), \n",
    "                                                       case=False, \n",
    "                                                       na=False)    \n",
    "    if state is not None:\n",
    "        if 'STATE' not in df.columns:\n",
    "            print(f\"Warning: STATE column not found, cannot filter by state\")\n",
    "        else:\n",
    "            state_mask = df['STATE'].str.upper() == state.upper()\n",
    "            mask = mask & state_mask\n",
    "    \n",
    "    df = df[mask].copy()\n",
    "\n",
    "    # Remove rows with invalid coordinates\n",
    "    if 'BEGIN_LAT' in df.columns and 'BEGIN_LON' in df.columns:\n",
    "        df['BEGIN_LAT'] = pd.to_numeric(df['BEGIN_LAT'], errors='coerce')\n",
    "        df['BEGIN_LON'] = pd.to_numeric(df['BEGIN_LON'], errors='coerce')\n",
    "        mask = (df['BEGIN_LAT'].notna()) & (df['BEGIN_LON'].notna()) & \\\n",
    "            (df['BEGIN_LAT'] != 0) & (df['BEGIN_LON'] != 0)\n",
    "        \n",
    "        filtered_df = df[mask].copy()\n",
    "    else:\n",
    "        filtered_df = df.copy()\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_yearmonth(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    if df.empty or 'BEGIN_YEARMONTH' not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['YEAR'] = df['BEGIN_YEARMONTH'].astype(str).str[:4].astype(int)\n",
    "    df['MONTH'] = df['BEGIN_YEARMONTH'].astype(str).str[4:6].astype(int)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    columns_to_keep = [\n",
    "        'YEAR',\n",
    "        'MONTH',\n",
    "        'BEGIN_DAY',\n",
    "        'BEGIN_TIME',\n",
    "        'BEGIN_LAT',\n",
    "        'BEGIN_LON',\n",
    "        'STATE',\n",
    "        'EVENT_TYPE',\n",
    "        'FLOOD_CAUSE',\n",
    "        'EVENT_NARRATIVE'\n",
    "    ]\n",
    "    \n",
    "\n",
    "    available_columns = [col for col in columns_to_keep if col in df.columns]\n",
    "    missing_columns = [col for col in columns_to_keep if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"Warning: The following columns were not found: {missing_columns}\")\n",
    "    \n",
    "    if not available_columns:\n",
    "        print(\"Warning: None of the requested columns were found in the DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    selected_df = df[available_columns].copy()\n",
    "    return selected_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_flooding_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    from datetime import datetime, timedelta\n",
    "    def modify_date(row):\n",
    "        try:\n",
    "            original_date = datetime(int(row['YEAR']), int(row['MONTH']), int(row['BEGIN_DAY']))\n",
    "            new_date = original_date - timedelta(days=10)\n",
    "            return new_date\n",
    "        except (ValueError, KeyError) as e:\n",
    "            return None\n",
    "    \n",
    "    modified_dates = df.apply(modify_date, axis=1)\n",
    "    df.loc[modified_dates.notna(), 'YEAR'] = modified_dates[modified_dates.notna()].apply(lambda x: x.year)\n",
    "    df.loc[modified_dates.notna(), 'MONTH'] = modified_dates[modified_dates.notna()].apply(lambda x: x.month)\n",
    "    df.loc[modified_dates.notna(), 'BEGIN_DAY'] = modified_dates[modified_dates.notna()].apply(lambda x: x.day)\n",
    "    \n",
    "    # Change event type to 'Normal'\n",
    "    if 'EVENT_TYPE' in df.columns:\n",
    "        df['EVENT_TYPE'] = 'Normal'\n",
    "    \n",
    "    # Change flood cause to empty string\n",
    "    if 'FLOOD_CAUSE' in df.columns:\n",
    "        df['FLOOD_CAUSE'] = ''\n",
    "    \n",
    "    # Change event narrative to empty string\n",
    "    if 'EVENT_NARRATIVE' in df.columns:\n",
    "        df['EVENT_NARRATIVE'] = ''\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_csv_files(raw_data_dir: str, state: str = None) -> pd.DataFrame:\n",
    "    csv_files = get_csv_files(raw_data_dir)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {raw_data_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV file(s) to process\")\n",
    "    if state:\n",
    "        print(f\"Filtering for state: {state.upper()}\")\n",
    "    \n",
    "    all_flooding_events = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Processing {os.path.basename(csv_file)}...\")\n",
    "        df = parse_csv_file(csv_file)\n",
    "        \n",
    "        if not df.empty:\n",
    "            flooding_df = filter_flooding_events(df, state=state)\n",
    "            if not flooding_df.empty:\n",
    "                print(f\"  Found {len(flooding_df)} flooding event(s)\")\n",
    "                all_flooding_events.append(flooding_df)\n",
    "            else:\n",
    "                print(f\"  No flooding events found\")\n",
    "        else:\n",
    "            print(f\"  Failed to parse or file is empty\")\n",
    "    \n",
    "    if all_flooding_events:\n",
    "        combined_df = pd.concat(all_flooding_events, ignore_index=True)\n",
    "        print(f\"\\nTotal flooding events found: {len(combined_df)}\")\n",
    "        \n",
    "        combined_df = split_yearmonth(combined_df)\n",
    "        selected_df = select_columns(combined_df)\n",
    "        selected_df = modify_flooding_data(selected_df)\n",
    "        return selected_df\n",
    "    else:\n",
    "        print(\"\\nNo flooding events found in any files\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: raw_data\n",
      "State filter: TEXAS\n"
     ]
    }
   ],
   "source": [
    "raw_data_dir = \"raw_data\"\n",
    "state_filter = \"TEXAS\" \n",
    "\n",
    "print(f\"Data directory: {raw_data_dir}\")\n",
    "print(f\"State filter: {state_filter if state_filter else 'None (all states)'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 CSV file(s) to process\n",
      "Filtering for state: TEXAS\n",
      "Processing StormEvents_details-ftp_v1.0_d2001_c20250520.csv...\n",
      "  Found 44 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2002_c20250520.csv...\n",
      "  No flooding events found\n",
      "Processing StormEvents_details-ftp_v1.0_d2003_c20250520.csv...\n",
      "  Found 45 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2004_c20250520.csv...\n",
      "  Found 32 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2005_c20250520.csv...\n",
      "  Found 374 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2006_c20250520.csv...\n",
      "  Found 136 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2007_c20250520.csv...\n",
      "  Found 1345 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2008_c20250520.csv...\n",
      "  Found 243 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2009_c20250520.csv...\n",
      "  Found 533 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2010_c20250520.csv...\n",
      "  Found 416 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2011_c20250520.csv...\n",
      "  Found 47 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2012_c20250520.csv...\n",
      "  Found 335 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2013_c20250520.csv...\n",
      "  Found 278 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2014_c20250520.csv...\n",
      "  Found 344 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2015_c20250818.csv...\n",
      "  Found 1196 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2016_c20250818.csv...\n",
      "  Found 692 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2017_c20250520.csv...\n",
      "  Found 424 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2018_c20250520.csv...\n",
      "  Found 501 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2019_c20250520.csv...\n",
      "  Found 321 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2020_c20250702.csv...\n",
      "  Found 230 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2021_c20250520.csv...\n",
      "  Found 503 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2022_c20250721.csv...\n",
      "  Found 214 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2023_c20250731.csv...\n",
      "  Found 267 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2024_c20250818.csv...\n",
      "  Found 578 flooding event(s)\n",
      "Processing StormEvents_details-ftp_v1.0_d2025_c20250818.csv...\n",
      "  Found 242 flooding event(s)\n",
      "\n",
      "Total flooding events found: 9340\n",
      "   YEAR  MONTH  BEGIN_DAY  BEGIN_TIME  BEGIN_LAT  BEGIN_LON  STATE  \\\n",
      "0  2001      3         11         140   32.53333  -96.66667  TEXAS   \n",
      "1  2001      5          4          15   33.73333 -102.78333  TEXAS   \n",
      "2  2001      5          6         125   31.46667  -97.71667  TEXAS   \n",
      "3  2001      5          6         128   33.08333  -97.13333  TEXAS   \n",
      "4  2001      5          6        1700   33.36667  -97.68333  TEXAS   \n",
      "\n",
      "    EVENT_TYPE FLOOD_CAUSE                                    EVENT_NARRATIVE  \n",
      "0  Flash Flood         NaN   Roads were closed near Sanger due to high water.  \n",
      "1  Flash Flood         NaN  I-27 was flooded and subsequently closed due t...  \n",
      "2  Flash Flood         NaN  Water was over Northwest Highway at Hillcrest ...  \n",
      "3  Flash Flood         NaN             Water was over Highway 75 at Parkview.  \n",
      "4  Flash Flood         NaN  Flooding resulted in high water rescues and ma...  \n",
      "   YEAR  MONTH  BEGIN_DAY  BEGIN_TIME  BEGIN_LAT  BEGIN_LON  STATE EVENT_TYPE  \\\n",
      "0  2001      3          6         140   32.53333  -96.66667  TEXAS     Normal   \n",
      "1  2001      4         29          15   33.73333 -102.78333  TEXAS     Normal   \n",
      "2  2001      5          1         125   31.46667  -97.71667  TEXAS     Normal   \n",
      "3  2001      5          1         128   33.08333  -97.13333  TEXAS     Normal   \n",
      "4  2001      5          1        1700   33.36667  -97.68333  TEXAS     Normal   \n",
      "\n",
      "  FLOOD_CAUSE EVENT_NARRATIVE  \n",
      "0                              \n",
      "1                              \n",
      "2                              \n",
      "3                              \n",
      "4                              \n"
     ]
    }
   ],
   "source": [
    "non_flooding_events_df = process_all_csv_files(str(raw_data_dir), state=state_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows: 9340\n",
      "\n",
      "Columns: ['YEAR', 'MONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'BEGIN_LAT', 'BEGIN_LON', 'STATE', 'EVENT_TYPE', 'FLOOD_CAUSE', 'EVENT_NARRATIVE']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>STATE</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>FLOOD_CAUSE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>140</td>\n",
       "      <td>32.53333</td>\n",
       "      <td>-96.66667</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>33.73333</td>\n",
       "      <td>-102.78333</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>31.46667</td>\n",
       "      <td>-97.71667</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>33.08333</td>\n",
       "      <td>-97.13333</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>33.36667</td>\n",
       "      <td>-97.68333</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>33.75000</td>\n",
       "      <td>-96.55000</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>2330</td>\n",
       "      <td>32.05000</td>\n",
       "      <td>-97.20000</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32.03333</td>\n",
       "      <td>-97.13333</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>29.36667</td>\n",
       "      <td>-95.08333</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>430</td>\n",
       "      <td>30.50000</td>\n",
       "      <td>-95.33333</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  MONTH  BEGIN_DAY  BEGIN_TIME  BEGIN_LAT  BEGIN_LON  STATE EVENT_TYPE  \\\n",
       "0  2001      3          6         140   32.53333  -96.66667  TEXAS     Normal   \n",
       "1  2001      4         29          15   33.73333 -102.78333  TEXAS     Normal   \n",
       "2  2001      5          1         125   31.46667  -97.71667  TEXAS     Normal   \n",
       "3  2001      5          1         128   33.08333  -97.13333  TEXAS     Normal   \n",
       "4  2001      5          1        1700   33.36667  -97.68333  TEXAS     Normal   \n",
       "5  2001      5          1          45   33.75000  -96.55000  TEXAS     Normal   \n",
       "6  2001      4         30        2330   32.05000  -97.20000  TEXAS     Normal   \n",
       "7  2001      5          1           0   32.03333  -97.13333  TEXAS     Normal   \n",
       "8  2001      6          2        2000   29.36667  -95.08333  TEXAS     Normal   \n",
       "9  2001      6          3         430   30.50000  -95.33333  TEXAS     Normal   \n",
       "\n",
       "  FLOOD_CAUSE EVENT_NARRATIVE  \n",
       "0                              \n",
       "1                              \n",
       "2                              \n",
       "3                              \n",
       "4                              \n",
       "5                              \n",
       "6                              \n",
       "7                              \n",
       "8                              \n",
       "9                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9340 entries, 0 to 9339\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   YEAR             9340 non-null   int64  \n",
      " 1   MONTH            9340 non-null   int64  \n",
      " 2   BEGIN_DAY        9340 non-null   int64  \n",
      " 3   BEGIN_TIME       9340 non-null   int64  \n",
      " 4   BEGIN_LAT        9340 non-null   float64\n",
      " 5   BEGIN_LON        9340 non-null   float64\n",
      " 6   STATE            9340 non-null   object \n",
      " 7   EVENT_TYPE       9340 non-null   object \n",
      " 8   FLOOD_CAUSE      9340 non-null   object \n",
      " 9   EVENT_NARRATIVE  9340 non-null   object \n",
      "dtypes: float64(2), int64(4), object(4)\n",
      "memory usage: 729.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display summary information\n",
    "if not non_flooding_events_df.empty:\n",
    "    print(f\"\\nTotal rows: {len(non_flooding_events_df)}\")\n",
    "    print(f\"\\nColumns: {list(non_flooding_events_df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(non_flooding_events_df.head(10))\n",
    "    \n",
    "    print(f\"\\nDataFrame info:\")\n",
    "    non_flooding_events_df.info()\n",
    "else:\n",
    "    print(\"No flooding events found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>STATE</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>FLOOD_CAUSE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22180</th>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>29.41000</td>\n",
       "      <td>-100.90000</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Hail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32610</th>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2040</td>\n",
       "      <td>35.24000</td>\n",
       "      <td>-100.16000</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Hail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Law enforcement received golf ball size hail 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1410</td>\n",
       "      <td>30.10000</td>\n",
       "      <td>-96.08333</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Hail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>2001</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1945</td>\n",
       "      <td>33.21667</td>\n",
       "      <td>-97.15000</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Large tree limbs were blown off trees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37062</th>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>30.41000</td>\n",
       "      <td>-98.74000</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A thunderstorm produced wind gusts estimated a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       YEAR  MONTH  BEGIN_DAY  BEGIN_TIME  BEGIN_LAT  BEGIN_LON  STATE  \\\n",
       "22180  2010      5          1          18   29.41000 -100.90000  TEXAS   \n",
       "32610  2015      4         11        2040   35.24000 -100.16000  TEXAS   \n",
       "150    2001      3         14        1410   30.10000  -96.08333  TEXAS   \n",
       "2098   2001     10         12        1945   33.21667  -97.15000  TEXAS   \n",
       "37062  2016      4         27          45   30.41000  -98.74000  TEXAS   \n",
       "\n",
       "              EVENT_TYPE FLOOD_CAUSE  \\\n",
       "22180               Hail         NaN   \n",
       "32610               Hail         NaN   \n",
       "150                 Hail         NaN   \n",
       "2098   Thunderstorm Wind         NaN   \n",
       "37062  Thunderstorm Wind         NaN   \n",
       "\n",
       "                                         EVENT_NARRATIVE  \n",
       "22180                                                NaN  \n",
       "32610  Law enforcement received golf ball size hail 5...  \n",
       "150                                                  NaN  \n",
       "2098              Large tree limbs were blown off trees.  \n",
       "37062  A thunderstorm produced wind gusts estimated a...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a random subset of 10000 rows from the non_flooding_events_df\n",
    "non_flooding_events_df = non_flooding_events_df.sample(n=9500)\n",
    "non_flooding_events_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openmeteo-requests in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (1.7.4)\n",
      "Requirement already satisfied: niquests>=3.15.2 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from openmeteo-requests) (3.15.2)\n",
      "Requirement already satisfied: openmeteo-sdk>=1.22.0 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from openmeteo-requests) (1.23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from niquests>=3.15.2->openmeteo-requests) (3.4.4)\n",
      "Requirement already satisfied: urllib3-future<3,>=2.13.903 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from niquests>=3.15.2->openmeteo-requests) (2.14.907)\n",
      "Requirement already satisfied: wassima<3,>=1.0.1 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from niquests>=3.15.2->openmeteo-requests) (2.0.2)\n",
      "Requirement already satisfied: h11<1.0.0,>=0.11.0 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests) (0.16.0)\n",
      "Requirement already satisfied: jh2<6.0.0,>=5.0.3 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests) (5.0.10)\n",
      "Requirement already satisfied: qh3<2.0.0,>=1.5.4 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests) (1.5.6)\n",
      "Requirement already satisfied: flatbuffers==25.9.23 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from openmeteo-sdk>=1.22.0->openmeteo-requests) (25.9.23)\n",
      "Requirement already satisfied: requests-cache in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: retry-requests in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: attrs>=21.2 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from requests-cache) (25.4.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from requests-cache) (25.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from requests-cache) (4.5.0)\n",
      "Requirement already satisfied: requests>=2.22 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from requests-cache) (2.32.5)\n",
      "Requirement already satisfied: url-normalize>=1.4 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from requests-cache) (2.2.1)\n",
      "Requirement already satisfied: urllib3>=1.25.5 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from requests-cache) (2.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.1.1 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from cattrs>=22.2->requests-cache) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.14.0 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from cattrs>=22.2->requests-cache) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from requests>=2.22->requests-cache) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from requests>=2.22->requests-cache) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ananthkothuri/VSCode/GeospatialAIProject/.venv/lib/python3.10/site-packages (from requests>=2.22->requests-cache) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openmeteo-requests\n",
    "!pip3 install requests-cache retry-requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting flooding data using the [Open Meteo API](https://open-meteo.com/en/docs/climate-api?utm_source=chatgpt.com&daily=temperature_2m_mean,wind_speed_10m_mean,cloud_cover_mean,relative_humidity_2m_mean,dew_point_2m_mean,precipitation_sum,rain_sum,snowfall_sum,pressure_msl_mean,soil_moisture_0_to_10cm_mean&start_date=2020-01-01&end_date=2020-01-01&models=EC_Earth3P_HR#settings)\n",
    "\n",
    "This is an open-sourced api that allows you to entire a lat, long, and date and it gives you data on the weather conditions of that area at the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "def get_weather_data(lat: float, lon: float, date: str) -> dict:\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "    url = \"https://climate-api.open-meteo.com/v1/climate\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": date,\n",
    "        \"end_date\": date,\n",
    "        \"models\": \"EC_Earth3P_HR\",\n",
    "        \"daily\": [\"temperature_2m_mean\", \"wind_speed_10m_mean\", \"cloud_cover_mean\", \"relative_humidity_2m_mean\", \"dew_point_2m_mean\", \"precipitation_sum\", \"rain_sum\", \"snowfall_sum\", \"pressure_msl_mean\", \"soil_moisture_0_to_10cm_mean\"],\n",
    "        \"utm_source\": \"chatgpt.com\",\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    daily = response.Daily()\n",
    "    daily_temperature_2m_mean = daily.Variables(0).ValuesAsNumpy()\n",
    "    daily_wind_speed_10m_mean = daily.Variables(1).ValuesAsNumpy()\n",
    "    daily_cloud_cover_mean = daily.Variables(2).ValuesAsNumpy()\n",
    "    daily_relative_humidity_2m_mean = daily.Variables(3).ValuesAsNumpy()\n",
    "    daily_dew_point_2m_mean = daily.Variables(4).ValuesAsNumpy()\n",
    "    daily_precipitation_sum = daily.Variables(5).ValuesAsNumpy()\n",
    "    daily_rain_sum = daily.Variables(6).ValuesAsNumpy()\n",
    "    daily_snowfall_sum = daily.Variables(7).ValuesAsNumpy()\n",
    "    daily_pressure_msl_mean = daily.Variables(8).ValuesAsNumpy()\n",
    "    daily_soil_moisture_0_to_10cm_mean = daily.Variables(9).ValuesAsNumpy()\n",
    "\n",
    "    daily_data = {\"date\": date}\n",
    "\n",
    "    daily_data[\"temperature_2m_mean\"] = daily_temperature_2m_mean\n",
    "    daily_data[\"wind_speed_10m_mean\"] = daily_wind_speed_10m_mean\n",
    "    daily_data[\"cloud_cover_mean\"] = daily_cloud_cover_mean\n",
    "    daily_data[\"relative_humidity_2m_mean\"] = daily_relative_humidity_2m_mean\n",
    "    daily_data[\"dew_point_2m_mean\"] = daily_dew_point_2m_mean\n",
    "    daily_data[\"precipitation_sum\"] = daily_precipitation_sum\n",
    "    daily_data[\"rain_sum\"] = daily_rain_sum\n",
    "    daily_data[\"snowfall_sum\"] = daily_snowfall_sum\n",
    "    daily_data[\"pressure_msl_mean\"] = daily_pressure_msl_mean\n",
    "    daily_data[\"soil_moisture_0_to_10cm_mean\"] = daily_soil_moisture_0_to_10cm_mean\n",
    "    daily_data[\"elevation\"] = response.Elevation()\n",
    "\n",
    "    daily_dataframe = pd.DataFrame(data = daily_data)\n",
    "    return daily_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting 9500 flooding events with weather data...\n",
      "This may take a while due to API rate limits...\n",
      "\n",
      "Processed 50/9500 events...\n",
      "Processed 100/9500 events...\n",
      "Processed 150/9500 events...\n",
      "Processed 200/9500 events...\n",
      "Processed 250/9500 events...\n",
      "Processed 300/9500 events...\n",
      "Processed 350/9500 events...\n",
      "Processed 400/9500 events...\n",
      "Processed 450/9500 events...\n",
      "Processed 500/9500 events...\n",
      "Processed 550/9500 events...\n",
      "Processed 600/9500 events...\n",
      "Processed 650/9500 events...\n",
      "Processed 700/9500 events...\n",
      "Processed 750/9500 events...\n",
      "Processed 800/9500 events...\n",
      "Processed 850/9500 events...\n",
      "Processed 900/9500 events...\n",
      "Processed 950/9500 events...\n",
      "Processed 1000/9500 events...\n",
      "Processed 1050/9500 events...\n",
      "Processed 1100/9500 events...\n",
      "Processed 1150/9500 events...\n",
      "Processed 1200/9500 events...\n",
      "Processed 1250/9500 events...\n",
      "Processed 1300/9500 events...\n",
      "Processed 1350/9500 events...\n",
      "Processed 1400/9500 events...\n",
      "Processed 1450/9500 events...\n",
      "Processed 1500/9500 events...\n",
      "Processed 1550/9500 events...\n",
      "Processed 1600/9500 events...\n",
      "Processed 1650/9500 events...\n",
      "Processed 1700/9500 events...\n",
      "Processed 1750/9500 events...\n",
      "Processed 1800/9500 events...\n",
      "Processed 1850/9500 events...\n",
      "Processed 1900/9500 events...\n",
      "Processed 1950/9500 events...\n",
      "Processed 2000/9500 events...\n",
      "Processed 2050/9500 events...\n",
      "Processed 2100/9500 events...\n",
      "Processed 2150/9500 events...\n",
      "Processed 2200/9500 events...\n",
      "Processed 2250/9500 events...\n",
      "Processed 2300/9500 events...\n",
      "Processed 2350/9500 events...\n",
      "Processed 2400/9500 events...\n",
      "Processed 2450/9500 events...\n",
      "Processed 2500/9500 events...\n",
      "Processed 2550/9500 events...\n",
      "Processed 2600/9500 events...\n",
      "Processed 2650/9500 events...\n",
      "Processed 2700/9500 events...\n",
      "Processed 2750/9500 events...\n",
      "Processed 2800/9500 events...\n",
      "Processed 2850/9500 events...\n",
      "Processed 2900/9500 events...\n",
      "Processed 2950/9500 events...\n",
      "Processed 3000/9500 events...\n",
      "Processed 3050/9500 events...\n",
      "Processed 3100/9500 events...\n",
      "Processed 3150/9500 events...\n",
      "Processed 3200/9500 events...\n",
      "Processed 3250/9500 events...\n",
      "Processed 3300/9500 events...\n",
      "Processed 3350/9500 events...\n",
      "Processed 3400/9500 events...\n",
      "Processed 3450/9500 events...\n",
      "Processed 3500/9500 events...\n",
      "Processed 3550/9500 events...\n",
      "Processed 3600/9500 events...\n",
      "Processed 3650/9500 events...\n",
      "Processed 3700/9500 events...\n",
      "Processed 3750/9500 events...\n",
      "Processed 3800/9500 events...\n",
      "Processed 3850/9500 events...\n",
      "Processed 3900/9500 events...\n",
      "Processed 3950/9500 events...\n",
      "Processed 4000/9500 events...\n",
      "Processed 4050/9500 events...\n",
      "Processed 4100/9500 events...\n",
      "Processed 4150/9500 events...\n",
      "Processed 4200/9500 events...\n",
      "Processed 4250/9500 events...\n",
      "Processed 4300/9500 events...\n",
      "Processed 4350/9500 events...\n",
      "Processed 4400/9500 events...\n",
      "Processed 4450/9500 events...\n",
      "Processed 4500/9500 events...\n",
      "Processed 4550/9500 events...\n",
      "Processed 4600/9500 events...\n",
      "Processed 4650/9500 events...\n",
      "Processed 4700/9500 events...\n",
      "Processed 4750/9500 events...\n",
      "Processed 4800/9500 events...\n",
      "Processed 4850/9500 events...\n",
      "Processed 4900/9500 events...\n",
      "Processed 4950/9500 events...\n",
      "Processed 5000/9500 events...\n",
      "Processed 5050/9500 events...\n",
      "Processed 5100/9500 events...\n",
      "Processed 5150/9500 events...\n",
      "Processed 5200/9500 events...\n",
      "Processed 5250/9500 events...\n",
      "Processed 5300/9500 events...\n",
      "Processed 5350/9500 events...\n",
      "Processed 5400/9500 events...\n",
      "Processed 5450/9500 events...\n",
      "Processed 5500/9500 events...\n",
      "Processed 5550/9500 events...\n",
      "Processed 5600/9500 events...\n",
      "Processed 5650/9500 events...\n",
      "Processed 5700/9500 events...\n",
      "Processed 5750/9500 events...\n",
      "Processed 5800/9500 events...\n",
      "Processed 5850/9500 events...\n",
      "Processed 5900/9500 events...\n",
      "Processed 5950/9500 events...\n",
      "Processed 6000/9500 events...\n",
      "Processed 6050/9500 events...\n",
      "Processed 6100/9500 events...\n",
      "Processed 6150/9500 events...\n",
      "Processed 6200/9500 events...\n",
      "Processed 6250/9500 events...\n",
      "Processed 6300/9500 events...\n",
      "Processed 6350/9500 events...\n",
      "Processed 6400/9500 events...\n",
      "Processed 6450/9500 events...\n",
      "Processed 6500/9500 events...\n",
      "Processed 6550/9500 events...\n",
      "Processed 6600/9500 events...\n",
      "Processed 6650/9500 events...\n",
      "Processed 6700/9500 events...\n",
      "Processed 6750/9500 events...\n",
      "Processed 6800/9500 events...\n",
      "Processed 6850/9500 events...\n",
      "Processed 6900/9500 events...\n",
      "Processed 6950/9500 events...\n",
      "Processed 7000/9500 events...\n",
      "Processed 7050/9500 events...\n",
      "Processed 7100/9500 events...\n",
      "Processed 7150/9500 events...\n",
      "Processed 7200/9500 events...\n",
      "Processed 7250/9500 events...\n",
      "Processed 7300/9500 events...\n",
      "Processed 7350/9500 events...\n",
      "Processed 7400/9500 events...\n",
      "Processed 7450/9500 events...\n",
      "Processed 7500/9500 events...\n",
      "Processed 7550/9500 events...\n",
      "Processed 7600/9500 events...\n",
      "Processed 7650/9500 events...\n",
      "Processed 7700/9500 events...\n",
      "Processed 7750/9500 events...\n",
      "Processed 7800/9500 events...\n",
      "Processed 7850/9500 events...\n",
      "Processed 7900/9500 events...\n",
      "Processed 7950/9500 events...\n",
      "Processed 8000/9500 events...\n",
      "Processed 8050/9500 events...\n",
      "Processed 8100/9500 events...\n",
      "Processed 8150/9500 events...\n",
      "Processed 8200/9500 events...\n",
      "Processed 8250/9500 events...\n",
      "Processed 8300/9500 events...\n",
      "Processed 8350/9500 events...\n",
      "Processed 8400/9500 events...\n",
      "Processed 8450/9500 events...\n",
      "Processed 8500/9500 events...\n",
      "Processed 8550/9500 events...\n",
      "Processed 8600/9500 events...\n",
      "Processed 8650/9500 events...\n",
      "Processed 8700/9500 events...\n",
      "Processed 8750/9500 events...\n",
      "Processed 8800/9500 events...\n",
      "Processed 8850/9500 events...\n",
      "Processed 8900/9500 events...\n",
      "Processed 8950/9500 events...\n",
      "Processed 9000/9500 events...\n",
      "Processed 9050/9500 events...\n",
      "Processed 9100/9500 events...\n",
      "Processed 9150/9500 events...\n",
      "Processed 9200/9500 events...\n",
      "Processed 9250/9500 events...\n",
      "Processed 9300/9500 events...\n",
      "Processed 9350/9500 events...\n",
      "Processed 9400/9500 events...\n",
      "Processed 9450/9500 events...\n",
      "Processed 9500/9500 events...\n",
      "\n",
      "Augmentation complete! Processed 9500 events.\n",
      "\n",
      "Augmented DataFrame shape: (9500, 21)\n",
      "\n",
      "New columns added: set()\n",
      "\n",
      "First few rows of augmented data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>STATE</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>FLOOD_CAUSE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speed_10m_mean</th>\n",
       "      <th>cloud_cover_mean</th>\n",
       "      <th>relative_humidity_2m_mean</th>\n",
       "      <th>dew_point_2m_mean</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>rain_sum</th>\n",
       "      <th>snowfall_sum</th>\n",
       "      <th>pressure_msl_mean</th>\n",
       "      <th>soil_moisture_0_to_10cm_mean</th>\n",
       "      <th>elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>29.41000</td>\n",
       "      <td>-100.90000</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Hail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.253766</td>\n",
       "      <td>28.487219</td>\n",
       "      <td>60.979492</td>\n",
       "      <td>13.821892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.899902</td>\n",
       "      <td>0.326249</td>\n",
       "      <td>318.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2040</td>\n",
       "      <td>35.24000</td>\n",
       "      <td>-100.16000</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Hail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Law enforcement received golf ball size hail 5...</td>\n",
       "      <td>...</td>\n",
       "      <td>16.718573</td>\n",
       "      <td>35.630985</td>\n",
       "      <td>75.888046</td>\n",
       "      <td>17.391108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1008.741272</td>\n",
       "      <td>0.273536</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1410</td>\n",
       "      <td>30.10000</td>\n",
       "      <td>-96.08333</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Hail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.252332</td>\n",
       "      <td>82.120888</td>\n",
       "      <td>85.798393</td>\n",
       "      <td>20.577196</td>\n",
       "      <td>1.197766</td>\n",
       "      <td>1.197766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.202942</td>\n",
       "      <td>0.259449</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1945</td>\n",
       "      <td>33.21667</td>\n",
       "      <td>-97.15000</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Large tree limbs were blown off trees.</td>\n",
       "      <td>...</td>\n",
       "      <td>23.663277</td>\n",
       "      <td>64.207565</td>\n",
       "      <td>78.247787</td>\n",
       "      <td>20.589581</td>\n",
       "      <td>0.267922</td>\n",
       "      <td>0.267922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012.636353</td>\n",
       "      <td>0.207057</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>30.41000</td>\n",
       "      <td>-98.74000</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A thunderstorm produced wind gusts estimated a...</td>\n",
       "      <td>...</td>\n",
       "      <td>17.052565</td>\n",
       "      <td>56.042210</td>\n",
       "      <td>29.054462</td>\n",
       "      <td>4.967474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1010.412048</td>\n",
       "      <td>0.146342</td>\n",
       "      <td>534.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows \u00d7 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  MONTH  BEGIN_DAY  BEGIN_TIME  BEGIN_LAT  BEGIN_LON  STATE  \\\n",
       "0  2010      5          1          18   29.41000 -100.90000  TEXAS   \n",
       "1  2015      4         11        2040   35.24000 -100.16000  TEXAS   \n",
       "2  2001      3         14        1410   30.10000  -96.08333  TEXAS   \n",
       "3  2001     10         12        1945   33.21667  -97.15000  TEXAS   \n",
       "4  2016      4         27          45   30.41000  -98.74000  TEXAS   \n",
       "\n",
       "          EVENT_TYPE  FLOOD_CAUSE  \\\n",
       "0               Hail          NaN   \n",
       "1               Hail          NaN   \n",
       "2               Hail          NaN   \n",
       "3  Thunderstorm Wind          NaN   \n",
       "4  Thunderstorm Wind          NaN   \n",
       "\n",
       "                                     EVENT_NARRATIVE  ...  \\\n",
       "0                                                NaN  ...   \n",
       "1  Law enforcement received golf ball size hail 5...  ...   \n",
       "2                                                NaN  ...   \n",
       "3             Large tree limbs were blown off trees.  ...   \n",
       "4  A thunderstorm produced wind gusts estimated a...  ...   \n",
       "\n",
       "   wind_speed_10m_mean  cloud_cover_mean  relative_humidity_2m_mean  \\\n",
       "0            10.253766         28.487219                  60.979492   \n",
       "1            16.718573         35.630985                  75.888046   \n",
       "2             8.252332         82.120888                  85.798393   \n",
       "3            23.663277         64.207565                  78.247787   \n",
       "4            17.052565         56.042210                  29.054462   \n",
       "\n",
       "   dew_point_2m_mean  precipitation_sum  rain_sum  snowfall_sum  \\\n",
       "0          13.821892           0.000000  0.000000           0.0   \n",
       "1          17.391108           0.000000  0.000000           0.0   \n",
       "2          20.577196           1.197766  1.197766           0.0   \n",
       "3          20.589581           0.267922  0.267922           0.0   \n",
       "4           4.967474           0.000000  0.000000           0.0   \n",
       "\n",
       "   pressure_msl_mean  soil_moisture_0_to_10cm_mean  elevation  \n",
       "0        1013.899902                      0.326249      318.0  \n",
       "1        1008.741272                      0.273536      711.0  \n",
       "2        1017.202942                      0.259449       72.0  \n",
       "3        1012.636353                      0.207057      214.0  \n",
       "4        1010.412048                      0.146342      534.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmented data saved to: non_flooding_events_augmented.csv\n"
     ]
    }
   ],
   "source": [
    "# For every flooding event, augment the row with the weather data\n",
    "# Add the weather data to the flooding data\n",
    "# Save the augmented data to a new csv file\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def augment_flooding_data_with_weather(flooding_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    augmented_rows = []\n",
    "    total_rows = len(flooding_df)\n",
    "    \n",
    "    print(f\"Augmenting {total_rows} flooding events with weather data...\")\n",
    "    print(\"This may take a while due to API rate limits...\\n\")\n",
    "    \n",
    "    count = 0\n",
    "    for _, row in flooding_df.iterrows():\n",
    "        try:\n",
    "            year = int(row['YEAR'])\n",
    "            month = int(row['MONTH'])\n",
    "            day = int(row['BEGIN_DAY'])\n",
    "            date_str = f\"{year}-{month:02d}-{day:02d}\"\n",
    "            \n",
    "            weather_df = get_weather_data(\n",
    "                lat=float(row['BEGIN_LAT']),\n",
    "                lon=float(row['BEGIN_LON']),\n",
    "                date=date_str\n",
    "            )\n",
    "            \n",
    "            if not weather_df.empty:\n",
    "                weather_row = weather_df.iloc[0]\n",
    "                augmented_row = row.to_dict()\n",
    "                \n",
    "                for col in weather_df.columns:\n",
    "                    if col != 'date':\n",
    "                        augmented_row[col] = weather_row[col]\n",
    "                augmented_rows.append(augmented_row)\n",
    "            else:\n",
    "                augmented_rows.append(row.to_dict())\n",
    "            \n",
    "            if (count + 1) % 50 == 0:\n",
    "                print(f\"Processed {count + 1}/{total_rows} events...\")\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {count}: {e}\")\n",
    "            augmented_rows.append(row.to_dict())\n",
    "        count += 1\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    \n",
    "    print(f\"\\nAugmentation complete! Processed {len(augmented_df)} events.\")\n",
    "    return augmented_df\n",
    "\n",
    "# Augment the flooding events with weather data\n",
    "augmented_non_flooding_df = augment_flooding_data_with_weather(non_flooding_events_df)\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nAugmented DataFrame shape: {augmented_non_flooding_df.shape}\")\n",
    "print(f\"\\nNew columns added: {set(augmented_non_flooding_df.columns) - set(augmented_non_flooding_df.columns)}\")\n",
    "print(f\"\\nFirst few rows of augmented data:\")\n",
    "display(augmented_non_flooding_df.head())\n",
    "\n",
    "# Save to CSV file\n",
    "output_path = \"non_flooding_events_augmented.csv\"\n",
    "augmented_non_flooding_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nAugmented data saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.974085</td>\n",
       "      <td>-99.778711</td>\n",
       "      <td>Foard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.266361</td>\n",
       "      <td>-98.399741</td>\n",
       "      <td>Blanco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.607504</td>\n",
       "      <td>-102.343092</td>\n",
       "      <td>Hockley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.705666</td>\n",
       "      <td>-98.683874</td>\n",
       "      <td>Llano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.530650</td>\n",
       "      <td>-101.734951</td>\n",
       "      <td>Swisher</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  YEAR MONTH BEGIN_DAY BEGIN_TIME  BEGIN_LAT   BEGIN_LON     NAME\n",
       "0  NaN   NaN       NaN        NaN  33.974085  -99.778711    Foard\n",
       "1  NaN   NaN       NaN        NaN  30.266361  -98.399741   Blanco\n",
       "2  NaN   NaN       NaN        NaN  33.607504 -102.343092  Hockley\n",
       "3  NaN   NaN       NaN        NaN  30.705666  -98.683874    Llano\n",
       "4  NaN   NaN       NaN        NaN  34.530650 -101.734951  Swisher"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get testing data for flooding map of texas counties\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "texas_county_centers = pd.read_csv(\"raw_data/Texas_Counties_Centroid_Map.csv\")\n",
    "BEGIN_TIME = 0\n",
    "NUM_DATES_PER_COUNTY = 30\n",
    "\n",
    "# Generate 30 evenly spaced dates across 2025\n",
    "start_date = datetime(2025, 1, 1)\n",
    "end_date = datetime(2025, 12, 31)\n",
    "total_days = (end_date - start_date).days\n",
    "# Create evenly spaced dates (including start and end)\n",
    "date_intervals = [start_date + timedelta(days=int(total_days * i / (NUM_DATES_PER_COUNTY - 1))) \n",
    "                  for i in range(NUM_DATES_PER_COUNTY)]\n",
    "\n",
    "# Create a DataFrame with the dates\n",
    "dates_df = pd.DataFrame({\n",
    "    'date': date_intervals\n",
    "})\n",
    "dates_df['YEAR'] = dates_df['date'].dt.year\n",
    "dates_df['MONTH'] = dates_df['date'].dt.month\n",
    "dates_df['BEGIN_DAY'] = dates_df['date'].dt.day\n",
    "dates_df['BEGIN_TIME'] = BEGIN_TIME\n",
    "dates_df = dates_df[['YEAR', 'MONTH', 'BEGIN_DAY', 'BEGIN_TIME']]\n",
    "\n",
    "# Prepare county data\n",
    "counties_df = texas_county_centers[[\"X (Lat)\", \"Y (Long)\", \"CNTY_NM\"]].copy()\n",
    "counties_df.rename(columns={\"X (Lat)\": \"BEGIN_LAT\", \"Y (Long)\": \"BEGIN_LON\", \"CNTY_NM\": \"NAME\"}, inplace=True)\n",
    "\n",
    "# Create a key for cross join\n",
    "dates_df['key'] = 1\n",
    "counties_df['key'] = 1\n",
    "\n",
    "# Cross join to create all combinations (each county with all 30 dates)\n",
    "testing_data = counties_df.merge(dates_df, on='key', how='outer').drop('key', axis=1)\n",
    "\n",
    "# Reorder columns to match expected format\n",
    "testing_data = testing_data[[\"YEAR\", \"MONTH\", \"BEGIN_DAY\", \"BEGIN_TIME\", \"BEGIN_LAT\", \"BEGIN_LON\", \"NAME\"]]\n",
    "\n",
    "print(f\"Created {len(testing_data)} rows ({len(texas_county_centers)} counties \u00d7 {NUM_DATES_PER_COUNTY} dates)\")\n",
    "testing_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}