{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da3664-f2d1-42dd-8bc8-5c86f70036c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flood events: 9308\n",
      "Non-flood events: 9308\n",
      "\n",
      "Creating temporal sequences:\n",
      "Total sequences created: 17516\n",
      "Sequence shape: (17516, 5, 36)\n",
      "\n",
      "Train set: 14012 sequences\n",
      "Test set: 3504 sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/geoai/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING LSTM MODEL\n",
      "Epoch 1/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5125 - loss: 0.6930 - val_accuracy: 0.5155 - val_loss: 0.6915\n",
      "Epoch 2/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5054 - loss: 0.6903 - val_accuracy: 0.4877 - val_loss: 0.6914\n",
      "Epoch 3/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5103 - loss: 0.6883 - val_accuracy: 0.5191 - val_loss: 0.6879\n",
      "Epoch 4/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5207 - loss: 0.6875 - val_accuracy: 0.5244 - val_loss: 0.6843\n",
      "Epoch 5/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5273 - loss: 0.6852 - val_accuracy: 0.5605 - val_loss: 0.6816\n",
      "Epoch 6/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5455 - loss: 0.6812 - val_accuracy: 0.5419 - val_loss: 0.6839\n",
      "Epoch 7/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5611 - loss: 0.6778 - val_accuracy: 0.5341 - val_loss: 0.6825\n",
      "Epoch 8/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5587 - loss: 0.6745 - val_accuracy: 0.5637 - val_loss: 0.6774\n",
      "Epoch 9/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5797 - loss: 0.6674 - val_accuracy: 0.5433 - val_loss: 0.6854\n",
      "Epoch 10/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5859 - loss: 0.6611 - val_accuracy: 0.5523 - val_loss: 0.6850\n",
      "Epoch 11/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5990 - loss: 0.6534 - val_accuracy: 0.5722 - val_loss: 0.6710\n",
      "Epoch 12/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6108 - loss: 0.6519 - val_accuracy: 0.5908 - val_loss: 0.6671\n",
      "Epoch 13/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6150 - loss: 0.6412 - val_accuracy: 0.5951 - val_loss: 0.6683\n",
      "Epoch 14/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6317 - loss: 0.6327 - val_accuracy: 0.5737 - val_loss: 0.6795\n",
      "Epoch 15/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6412 - loss: 0.6263 - val_accuracy: 0.5908 - val_loss: 0.6721\n",
      "Epoch 16/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6479 - loss: 0.6184 - val_accuracy: 0.5883 - val_loss: 0.6751\n",
      "Epoch 17/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6579 - loss: 0.6104 - val_accuracy: 0.5933 - val_loss: 0.6676\n",
      "Epoch 18/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6602 - loss: 0.6010 - val_accuracy: 0.6072 - val_loss: 0.6647\n",
      "Epoch 19/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6738 - loss: 0.5944 - val_accuracy: 0.5979 - val_loss: 0.6716\n",
      "Epoch 20/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6824 - loss: 0.5814 - val_accuracy: 0.6090 - val_loss: 0.6711\n",
      "Epoch 21/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6877 - loss: 0.5798 - val_accuracy: 0.6019 - val_loss: 0.6822\n",
      "Epoch 22/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6917 - loss: 0.5679 - val_accuracy: 0.6179 - val_loss: 0.6710\n",
      "Epoch 23/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7021 - loss: 0.5577 - val_accuracy: 0.6133 - val_loss: 0.6720\n",
      "Epoch 24/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7057 - loss: 0.5518 - val_accuracy: 0.6168 - val_loss: 0.6806\n",
      "Epoch 25/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7127 - loss: 0.5415 - val_accuracy: 0.6147 - val_loss: 0.6762\n",
      "Epoch 26/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7196 - loss: 0.5336 - val_accuracy: 0.6101 - val_loss: 0.6840\n",
      "Epoch 27/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7280 - loss: 0.5180 - val_accuracy: 0.6236 - val_loss: 0.7108\n",
      "Epoch 28/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7297 - loss: 0.5134 - val_accuracy: 0.6208 - val_loss: 0.7248\n",
      "Epoch 29/100\n",
      "\u001b[1m139/351\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7428 - loss: 0.4958"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# LOAD DATA\n",
    "flood_df = pd.read_csv('../data/cleaned_flood_data.csv')\n",
    "non_flood_df = pd.read_csv('../data/cleaned_non_flood_data.csv')\n",
    "\n",
    "print(f\"Flood events: {len(flood_df)}\")\n",
    "print(f\"Non-flood events: {len(non_flood_df)}\")\n",
    "\n",
    "# Combine\n",
    "full_df = pd.concat([flood_df, non_flood_df])\n",
    "# Remove columns that are not needed for modeing\n",
    "full_df = full_df.drop(['precipitation_sum', 'STATE', 'FLOOD_CAUSE', 'EVENT_NARRATIVE'], \n",
    "                        axis=1, errors='ignore')\n",
    "\n",
    "# CREATE TIME SEQUENCES\n",
    "\n",
    "# groups events by spatial grid cell and builds sliding windows of sequential months,\n",
    "#each sequence becomes one training sample for the LSTM\n",
    "def create_temporal_sequences(df, sequence_length=5, grid_size=0.5):\n",
    "    df = df.copy()\n",
    "    df['time_idx'] = df['YEAR'] * 12 + df['MONTH']\n",
    "    \n",
    "    # convert lat/lon into grid IDs\n",
    "    df['grid_lat'] = (df['BEGIN_LAT'] / grid_size).astype(int)\n",
    "    df['grid_lon'] = (df['BEGIN_LON'] / grid_size).astype(int)\n",
    "    df['grid_id'] = df['grid_lat'].astype(str) + '_' + df['grid_lon'].astype(str)\n",
    "\n",
    "    # features that will be used as inpt to the LSTM\n",
    "    feature_cols = [\n",
    "        'YEAR', 'MONTH', 'BEGIN_TIME', 'BEGIN_LAT', 'BEGIN_LON',\n",
    "        'temperature_2m_mean', 'wind_speed_10m_mean', 'cloud_cover_mean',\n",
    "        'relative_humidity_2m_mean', 'dew_point_2m_mean', 'rain_sum',\n",
    "        'pressure_msl_mean', 'soil_moisture_0_to_10cm_mean', 'elevation',\n",
    "        'is_primary_rain_season', 'is_secondary_rain_season',\n",
    "        'Flood_Zone_A', 'Flood_Zone_AE', 'Flood_Zone_AH', 'Flood_Zone_AO',\n",
    "        'Flood_Zone_AREA NOT INCLUDED', 'Flood_Zone_OPEN WATER', 'Flood_Zone_VE', 'Flood_Zone_X',\n",
    "        'Is_In_Floodplain_False', 'Is_In_Floodplain_True'\n",
    "    ]\n",
    "    \n",
    "    flood_cols = [col for col in df.columns if col.startswith('Flood_Zone_') or col.startswith('Is_In_Floodplain_')]\n",
    "    feature_cols.extend(flood_cols)\n",
    "    \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process each grid cell independently\n",
    "    for grid_id, group in df.groupby('grid_id'):\n",
    "        if len(group) < sequence_length:\n",
    "            continue\n",
    "\n",
    "        # Sorts\n",
    "        group = group.sort_values('time_idx').reset_index(drop=True)\n",
    "\n",
    "        # Build sliding windows of length sequence length\n",
    "        for i in range(len(group) - sequence_length + 1):\n",
    "            window = group.iloc[i:i+sequence_length]\n",
    "\n",
    "            # Extract sequence features\n",
    "            seq_features = window[feature_cols].values\n",
    "\n",
    "            # labels (1 = flood event in last timestep)\n",
    "            last_event = window.iloc[-1]\n",
    "            label = 1 if last_event['EVENT_TYPE'] == 'Flash Flood' else 0\n",
    "            \n",
    "            sequences.append(seq_features)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), feature_cols\n",
    "\n",
    "print(\"\\nCreating temporal sequences:\")\n",
    "X_sequences, y_sequences, feature_names = create_temporal_sequences(\n",
    "    full_df, \n",
    "    sequence_length=5,\n",
    "    grid_size=0.5\n",
    ")\n",
    "\n",
    "print(f\"Total sequences created: {len(X_sequences)}\")\n",
    "print(f\"Sequence shape: {X_sequences.shape}\")\n",
    "\n",
    "# TRAIN-TEST SPLIT\n",
    "# Split sequences and labels so the model can be evaluated fairly\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_sequences\n",
    ")\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[2])\n",
    "X_test_2d = X_test.reshape(-1, X_test.shape[2])\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_2d).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test_2d).reshape(X_test.shape)\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train_scaled)} sequences\")\n",
    "print(f\"Test set: {len(X_test_scaled)} sequences\")\n",
    "\n",
    "# BUILD LSTM MODEL\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "lstm_model = build_lstm_model(input_shape)\n",
    "\n",
    "\n",
    "# calculte class weights\n",
    "class_weight = {\n",
    "    0: len(y_train) / (2 * (y_train == 0).sum()),\n",
    "    1: len(y_train) / (2 * (y_train == 1).sum())\n",
    "}\n",
    "\n",
    "# Stop training early when validation loss stops improving\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# TRAIN MODEL\n",
    "print(\"TRAINING LSTM MODEL\")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# EVALUATE\n",
    "y_pred_proba = lstm_model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"LSTM MODEL RESULTS\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-Flood', 'Flood']))\n",
    "\n",
    "# PLOT ACCURACY\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_accuracy.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0b89d-768c-49cf-8daa-d807be4511c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(geoai)",
   "language": "python",
   "name": "geoai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
