{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07da3664-f2d1-42dd-8bc8-5c86f70036c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flood events: 9308\n",
      "Non-flood events: 9308\n",
      "\n",
      "Creating temporal sequences:\n",
      "Total sequences created: 17516\n",
      "Sequence shape: (17516, 5, 36)\n",
      "\n",
      "Train set: 14012 sequences\n",
      "Test set: 3504 sequences\n",
      "TRAINING LSTM MODEL\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/geoai/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5134 - loss: 0.6924 - val_accuracy: 0.4699 - val_loss: 0.6948\n",
      "Epoch 2/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5137 - loss: 0.6897 - val_accuracy: 0.5248 - val_loss: 0.6904\n",
      "Epoch 3/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5223 - loss: 0.6884 - val_accuracy: 0.5305 - val_loss: 0.6869\n",
      "Epoch 4/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5341 - loss: 0.6864 - val_accuracy: 0.5244 - val_loss: 0.6866\n",
      "Epoch 5/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5389 - loss: 0.6844 - val_accuracy: 0.5298 - val_loss: 0.6884\n",
      "Epoch 6/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5428 - loss: 0.6820 - val_accuracy: 0.5359 - val_loss: 0.6825\n",
      "Epoch 7/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5546 - loss: 0.6794 - val_accuracy: 0.5448 - val_loss: 0.6796\n",
      "Epoch 8/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5687 - loss: 0.6727 - val_accuracy: 0.5491 - val_loss: 0.6834\n",
      "Epoch 9/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5678 - loss: 0.6703 - val_accuracy: 0.5590 - val_loss: 0.6744\n",
      "Epoch 10/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5797 - loss: 0.6656 - val_accuracy: 0.5658 - val_loss: 0.6736\n",
      "Epoch 11/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5953 - loss: 0.6595 - val_accuracy: 0.5551 - val_loss: 0.6792\n",
      "Epoch 12/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6086 - loss: 0.6524 - val_accuracy: 0.5740 - val_loss: 0.6709\n",
      "Epoch 13/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6148 - loss: 0.6433 - val_accuracy: 0.5690 - val_loss: 0.6679\n",
      "Epoch 14/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6190 - loss: 0.6408 - val_accuracy: 0.5879 - val_loss: 0.6666\n",
      "Epoch 15/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6418 - loss: 0.6294 - val_accuracy: 0.6008 - val_loss: 0.6597\n",
      "Epoch 16/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6431 - loss: 0.6234 - val_accuracy: 0.5858 - val_loss: 0.6707\n",
      "Epoch 17/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6474 - loss: 0.6174 - val_accuracy: 0.5854 - val_loss: 0.6673\n",
      "Epoch 18/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6633 - loss: 0.6073 - val_accuracy: 0.6008 - val_loss: 0.6677\n",
      "Epoch 19/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6696 - loss: 0.5983 - val_accuracy: 0.6076 - val_loss: 0.6614\n",
      "Epoch 20/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6737 - loss: 0.5944 - val_accuracy: 0.6090 - val_loss: 0.6628\n",
      "Epoch 21/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6805 - loss: 0.5838 - val_accuracy: 0.5997 - val_loss: 0.6648\n",
      "Epoch 22/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6907 - loss: 0.5749 - val_accuracy: 0.6247 - val_loss: 0.6612\n",
      "Epoch 23/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6972 - loss: 0.5619 - val_accuracy: 0.6161 - val_loss: 0.6708\n",
      "Epoch 24/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7035 - loss: 0.5587 - val_accuracy: 0.6176 - val_loss: 0.6695\n",
      "Epoch 25/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7101 - loss: 0.5488 - val_accuracy: 0.6261 - val_loss: 0.6784\n",
      "Epoch 26/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7126 - loss: 0.5451 - val_accuracy: 0.6250 - val_loss: 0.6757\n",
      "Epoch 27/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7234 - loss: 0.5319 - val_accuracy: 0.6258 - val_loss: 0.6768\n",
      "Epoch 28/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7302 - loss: 0.5303 - val_accuracy: 0.6261 - val_loss: 0.6767\n",
      "Epoch 29/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7343 - loss: 0.5198 - val_accuracy: 0.6211 - val_loss: 0.6920\n",
      "Epoch 30/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7411 - loss: 0.5054 - val_accuracy: 0.6258 - val_loss: 0.7152\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "LSTM MODEL RESULTS\n",
      "Test Accuracy: 0.5762\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Flood       0.64      0.58      0.61      2002\n",
      "       Flood       0.51      0.57      0.53      1502\n",
      "\n",
      "    accuracy                           0.58      3504\n",
      "   macro avg       0.57      0.57      0.57      3504\n",
      "weighted avg       0.58      0.58      0.58      3504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# LOAD DATA\n",
    "flood_df = pd.read_csv('../data/cleaned_flood_data.csv')\n",
    "non_flood_df = pd.read_csv('../data/cleaned_non_flood_data.csv')\n",
    "\n",
    "print(f\"Flood events: {len(flood_df)}\")\n",
    "print(f\"Non-flood events: {len(non_flood_df)}\")\n",
    "\n",
    "# Combine\n",
    "full_df = pd.concat([flood_df, non_flood_df])\n",
    "# Remove columns that are not needed for modeing\n",
    "full_df = full_df.drop(['precipitation_sum', 'STATE', 'FLOOD_CAUSE', 'EVENT_NARRATIVE'], \n",
    "                        axis=1, errors='ignore')\n",
    "\n",
    "# CREATE TIME SEQUENCES\n",
    "\n",
    "# groups events by spatial grid cell and builds sliding windows of sequential months,\n",
    "#each sequence becomes one training sample for the LSTM\n",
    "def create_temporal_sequences(df, sequence_length=5, grid_size=0.5):\n",
    "    df = df.copy()\n",
    "    df['time_idx'] = df['YEAR'] * 12 + df['MONTH']\n",
    "    \n",
    "    # convert lat/lon into grid IDs\n",
    "    df['grid_lat'] = (df['BEGIN_LAT'] / grid_size).astype(int)\n",
    "    df['grid_lon'] = (df['BEGIN_LON'] / grid_size).astype(int)\n",
    "    df['grid_id'] = df['grid_lat'].astype(str) + '_' + df['grid_lon'].astype(str)\n",
    "\n",
    "    # features that will be used as inpt to the LSTM\n",
    "    feature_cols = [\n",
    "        'YEAR', 'MONTH', 'BEGIN_TIME', 'BEGIN_LAT', 'BEGIN_LON',\n",
    "        'temperature_2m_mean', 'wind_speed_10m_mean', 'cloud_cover_mean',\n",
    "        'relative_humidity_2m_mean', 'dew_point_2m_mean', 'rain_sum',\n",
    "        'pressure_msl_mean', 'soil_moisture_0_to_10cm_mean', 'elevation',\n",
    "        'is_primary_rain_season', 'is_secondary_rain_season',\n",
    "        'Flood_Zone_A', 'Flood_Zone_AE', 'Flood_Zone_AH', 'Flood_Zone_AO',\n",
    "        'Flood_Zone_AREA NOT INCLUDED', 'Flood_Zone_OPEN WATER', 'Flood_Zone_VE', 'Flood_Zone_X',\n",
    "        'Is_In_Floodplain_False', 'Is_In_Floodplain_True'\n",
    "    ]\n",
    "    \n",
    "    flood_cols = [col for col in df.columns if col.startswith('Flood_Zone_') or col.startswith('Is_In_Floodplain_')]\n",
    "    feature_cols.extend(flood_cols)\n",
    "    \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process each grid cell independently\n",
    "    for grid_id, group in df.groupby('grid_id'):\n",
    "        if len(group) < sequence_length:\n",
    "            continue\n",
    "\n",
    "        # Sorts\n",
    "        group = group.sort_values('time_idx').reset_index(drop=True)\n",
    "\n",
    "        # Build sliding windows of length sequence length\n",
    "        for i in range(len(group) - sequence_length + 1):\n",
    "            window = group.iloc[i:i+sequence_length]\n",
    "\n",
    "            # Extract sequence features\n",
    "            seq_features = window[feature_cols].values\n",
    "\n",
    "            # labels (1 = flood event in last timestep)\n",
    "            last_event = window.iloc[-1]\n",
    "            label = 1 if last_event['EVENT_TYPE'] == 'Flash Flood' else 0\n",
    "            \n",
    "            sequences.append(seq_features)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), feature_cols\n",
    "\n",
    "print(\"\\nCreating temporal sequences:\")\n",
    "X_sequences, y_sequences, feature_names = create_temporal_sequences(\n",
    "    full_df, \n",
    "    sequence_length=5,\n",
    "    grid_size=0.5\n",
    ")\n",
    "\n",
    "print(f\"Total sequences created: {len(X_sequences)}\")\n",
    "print(f\"Sequence shape: {X_sequences.shape}\")\n",
    "\n",
    "# TRAIN-TEST SPLIT\n",
    "# Split sequences and labels so the model can be evaluated fairly\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_sequences\n",
    ")\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[2])\n",
    "X_test_2d = X_test.reshape(-1, X_test.shape[2])\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_2d).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test_2d).reshape(X_test.shape)\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train_scaled)} sequences\")\n",
    "print(f\"Test set: {len(X_test_scaled)} sequences\")\n",
    "\n",
    "# BUILD LSTM MODEL\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "lstm_model = build_lstm_model(input_shape)\n",
    "\n",
    "\n",
    "# calculte class weights\n",
    "class_weight = {\n",
    "    0: len(y_train) / (2 * (y_train == 0).sum()),\n",
    "    1: len(y_train) / (2 * (y_train == 1).sum())\n",
    "}\n",
    "\n",
    "# Stop training early when validation loss stops improving\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# TRAIN MODEL\n",
    "print(\"TRAINING LSTM MODEL\")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# EVALUATE\n",
    "y_pred_proba = lstm_model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"LSTM MODEL RESULTS\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-Flood', 'Flood']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0b89d-768c-49cf-8daa-d807be4511c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(geoai)",
   "language": "python",
   "name": "geoai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
